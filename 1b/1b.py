# -*- coding: utf-8 -*-
"""Adobe1B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_EuSICp41lTd4a0_JMs8B8TaMga0_VfE
"""

#!pip install -q PyMuPDF sentence-transformers

import os
import json
import time
import re
from collections import Counter

# Make sure to install the required libraries:
# !pip install -q torch sentence-transformers PyMuPDF nltk
import fitz  # PyMuPDF
import nltk
from sentence_transformers import SentenceTransformer, util

# --- Configuration ---
MODEL_NAME = 'all-MiniLM-L6-v2'
TOP_K_SECTIONS = 10
NUM_REFINED_SENTENCES = 5

# --- CORRECTED NLTK DOWNLOAD BLOCK ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK 'punkt' model...")
    nltk.download('punkt')
    print("Download complete.")


def get_base_font_size(page):
    """Heuristically find the most common font size for body text on a page."""
    font_sizes = [span['size'] for block in page.get_text("dict")['blocks'] for line in block['lines'] for span in line['spans']]
    if not font_sizes:
        return 10.0 # Default fallback
    return Counter(font_sizes).most_common(1)[0][0]


def parse_pdfs_to_sections(doc_paths: list) -> list:
    """
    Parses a list of PDF files into structured sections based on font size.
    """
    print("Parsing PDFs into sections...")
    all_sections = []
    for doc_path in doc_paths:
        if not os.path.exists(doc_path):
            print(f"Warning: Document not found at {doc_path}. Skipping.")
            continue

        doc_name = os.path.basename(doc_path)
        doc = fitz.open(doc_path)

        current_section = {"title": "Introduction", "content": "", "doc_name": doc_name, "page_num": 1}

        for page_num, page in enumerate(doc, start=1):
            base_font_size = get_base_font_size(page)
            blocks = page.get_text("dict")["blocks"]

            for block in blocks:
                if 'lines' in block:
                    spans = block['lines'][0]['spans']
                    if not spans:
                        continue

                    first_span = spans[0]
                    font_size = first_span['size']
                    block_text = " ".join([line['spans'][0]['text'] for line in block['lines']]).strip()

                    if font_size > base_font_size * 1.15 and len(block_text.split()) < 15:
                        if current_section["content"].strip():
                            all_sections.append(current_section)

                        current_section = {
                            "title": block_text,
                            "content": "",
                            "doc_name": doc_name,
                            "page_num": page_num
                        }
                    else:
                        for line in block['lines']:
                            for span in line['spans']:
                                current_section["content"] += span['text'] + " "
                        current_section["content"] += "\n"

        if current_section["content"].strip():
            all_sections.append(current_section)

        print(f"  - Parsed {len(doc)} pages from {doc_name}")

    print(f"Total sections extracted: {len(all_sections)}")
    return all_sections


def analyze_document_collection(doc_paths: list, persona: str, job_to_be_done: str):
    """
    The main pipeline to perform persona-driven document analysis.
    """
    start_time = time.time()

    print(f"\nLoading model '{MODEL_NAME}'... (This may take a moment on first run)")
    model = SentenceTransformer(MODEL_NAME)

    sections = parse_pdfs_to_sections(doc_paths)
    if not sections:
        print("No sections were extracted. Exiting.")
        return

    query = f"Persona: {persona}. Task: {job_to_be_done}"
    query_embedding = model.encode(query, convert_to_tensor=True)

    print("\nEmbedding document sections...")
    section_contents = [s['content'] for s in sections]
    section_embeddings = model.encode(section_contents, convert_to_tensor=True)
    print("Embedding complete.")

    print("\nRanking sections by relevance...")
    similarities = util.cos_sim(query_embedding, section_embeddings)[0]

    ranked_sections = []
    for i, section in enumerate(sections):
        section['relevance_score'] = similarities[i].item()
        ranked_sections.append(section)

    ranked_sections.sort(key=lambda x: x['relevance_score'], reverse=True)
    print("Ranking complete.")

    print("\nGenerating final JSON output...")
    output = {
        "Metadata": {
            "input_documents": [os.path.basename(p) for p in doc_paths],
            "persona": persona,
            "job_to_be_done": job_to_be_done,
            "processing_timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        },
        "ExtractedSection": [],
        "SubSectionAnalysis": []
    }

    for i, section in enumerate(ranked_sections[:TOP_K_SECTIONS]):
        rank = i + 1

        output["ExtractedSection"].append({
            "Document": section['doc_name'],
            "PageNumber": section['page_num'],
            "SectionTitle": section['title'],
            "Importance_rank": rank
        })

        sentences = nltk.sent_tokenize(section['content'])
        if not sentences:
            refined_text = "Content could not be split into sentences."
        else:
            sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
            sentence_sims = util.cos_sim(query_embedding, sentence_embeddings)[0]

            top_sentence_indices = sentence_sims.argsort(descending=True)[:NUM_REFINED_SENTENCES]
            top_sentence_indices = sorted(top_sentence_indices)

            refined_text = " ".join([sentences[j] for j in top_sentence_indices])

        output["SubSectionAnalysis"].append({
            "Document": section['doc_name'],
            "SectionTitle": section['title'],
            "RefinedText": refined_text,
            "PageNumber": section['page_num']
        })

    end_time = time.time()
    print(f"\nAnalysis complete in {end_time - start_time:.2f} seconds.")

    return output


if __name__ == '__main__':
    DOCS_FOLDER = 'documents'
    if not os.path.exists(DOCS_FOLDER):
        os.makedirs(DOCS_FOLDER)
        print(f"Created a folder '{DOCS_FOLDER}'. Please place your PDFs inside it and run again.")
        exit()

    document_paths = [os.path.join(DOCS_FOLDER, f) for f in os.listdir(DOCS_FOLDER) if f.lower().endswith('.pdf')]

    if not document_paths:
        print(f"No PDF files found in the '{DOCS_FOLDER}' folder. Please add some PDFs to analyze.")
        exit()

    PERSONA = "Investment Analyst"
    JOB = "Analyze revenue trends, R&D investments, and market positioning strategies from company annual reports."

    final_output = analyze_document_collection(document_paths, PERSONA, JOB)

    if final_output:
        output_filename = "challenge_output.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=4)

        print(f"\nOutput successfully saved to {output_filename}")

# Cell 1: Updated Setup
!pip install -q PyMuPDF sentence-transformers

import os
import nltk

# Create the folder for your PDFs
os.makedirs('documents', exist_ok=True)

# Download NLTK data (both 'punkt' and 'punkt_tab')
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    print("Downloading NLTK resource files...")
    nltk.download('punkt')
    nltk.download('punkt_tab')

print("\nâœ… Setup complete. Please ensure your PDFs are in the 'documents' folder, then run the analysis cell.")

# Cell 2: Analysis

import os
import json
import time
from collections import Counter
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer, util

# --- Configuration ---
MODEL_NAME = 'all-MiniLM-L6-v2'
TOP_K_SECTIONS = 10
NUM_REFINED_SENTENCES = 5

def get_base_font_size(page):
    font_sizes = [span['size'] for block in page.get_text("dict")['blocks'] if 'lines' in block for line in block['lines'] for span in line['spans']]
    if not font_sizes:
        return 10.0
    return Counter(font_sizes).most_common(1)[0][0]

def parse_pdfs_to_sections(doc_paths: list) -> list:
    print("Parsing PDFs into sections...")
    all_sections = []
    for doc_path in doc_paths:
        if not os.path.exists(doc_path):
            print(f"Warning: Document not found at {doc_path}. Skipping.")
            continue
        doc_name = os.path.basename(doc_path)
        doc = fitz.open(doc_path)
        current_section = {"title": "Introduction", "content": "", "doc_name": doc_name, "page_num": 1}
        for page_num, page in enumerate(doc, start=1):
            base_font_size = get_base_font_size(page)
            blocks = page.get_text("dict")["blocks"]
            for block in blocks:
                if 'lines' in block and block['lines'] and block['lines'][0]['spans']:
                    first_span = block['lines'][0]['spans'][0]
                    font_size = first_span['size']
                    block_text = " ".join([span['text'] for line in block['lines'] for span in line['spans']]).strip()
                    if font_size > base_font_size * 1.15 and len(block_text.split()) < 20:
                        if current_section["content"].strip():
                            all_sections.append(current_section)
                        current_section = {"title": block_text, "content": "", "doc_name": doc_name, "page_num": page_num}
                    else:
                        current_section["content"] += block_text + "\n"
        if current_section["content"].strip():
            all_sections.append(current_section)
        print(f"  - Parsed {len(doc)} pages from {doc_name}")
    print(f"Total sections extracted: {len(all_sections)}")
    return all_sections

def analyze_document_collection(doc_paths: list, persona: str, job_to_be_done: str):
    start_time = time.time()
    print(f"\nLoading model '{MODEL_NAME}'...")
    model = SentenceTransformer(MODEL_NAME)
    sections = parse_pdfs_to_sections(doc_paths)
    if not sections:
        print("No sections were extracted. Exiting.")
        return
    query = f"Persona: {persona}. Task: {job_to_be_done}"
    query_embedding = model.encode(query, convert_to_tensor=True)
    print("\nEmbedding document sections...")
    section_contents = [s['content'] for s in sections]
    section_embeddings = model.encode(section_contents, convert_to_tensor=True)
    print("Embedding complete.")
    print("\nRanking sections by relevance...")
    similarities = util.cos_sim(query_embedding, section_embeddings)[0]
    ranked_sections = []
    for i, section in enumerate(sections):
        section['relevance_score'] = similarities[i].item()
        ranked_sections.append(section)
    ranked_sections.sort(key=lambda x: x['relevance_score'], reverse=True)
    print("Ranking complete.")
    print("\nGenerating final JSON output...")
    output = {
        "Metadata": {
            "input_documents": [os.path.basename(p) for p in doc_paths],
            "persona": persona,
            "job_to_be_done": job_to_be_done,
            "processing_timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        },
        "ExtractedSection": [],
        "SubSectionAnalysis": []
    }
    for i, section in enumerate(ranked_sections[:TOP_K_SECTIONS]):
        rank = i + 1
        output["ExtractedSection"].append({
            "Document": section['doc_name'], "PageNumber": section['page_num'],
            "SectionTitle": section['title'], "Importance_rank": rank
        })
        sentences = nltk.sent_tokenize(section['content'])
        if not sentences:
            refined_text = "Content could not be split into sentences."
        else:
            sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
            sentence_sims = util.cos_sim(query_embedding, sentence_embeddings)[0]
            top_sentence_indices = sorted(sentence_sims.argsort(descending=True)[:NUM_REFINED_SENTENCES])
            refined_text = " ".join([sentences[j] for j in top_sentence_indices])
        output["SubSectionAnalysis"].append({
            "Document": section['doc_name'], "SectionTitle": section['title'],
            "RefinedText": refined_text, "PageNumber": section['page_num']
        })
    end_time = time.time()
    print(f"\nAnalysis complete in {end_time - start_time:.2f} seconds.")
    return output

# --- Main Execution ---
DOCS_FOLDER = 'documents'
document_paths = [os.path.join(DOCS_FOLDER, f) for f in os.listdir(DOCS_FOLDER) if f.lower().endswith('.pdf')]

if not document_paths:
    print(f"No PDF files found in the '{DOCS_FOLDER}' folder. Please add your PDFs to analyze.")
else:
    PERSONA = "Investment Analyst"
    JOB = "Analyze revenue trends, R&D investments, and market positioning strategies from company annual reports."
    final_output = analyze_document_collection(document_paths, PERSONA, JOB)
    if final_output:
        output_filename = "challenge_output.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=4)
        print(f"\nOutput successfully saved to {output_filename}")
        # To display the output in Colab, you can print it
        # print("\n--- JSON OUTPUT ---")
        # print(json.dumps(final_output, indent=4))

